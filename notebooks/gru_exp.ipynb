{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2ac367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Train: 5798, Test: 1450\n",
      "\n",
      "======================================================================\n",
      " GRU MODEL WITH 5-FOLD CROSS-VALIDATION\n",
      "======================================================================\n",
      "\n",
      "Using device: cpu\n",
      "\n",
      "1. Preprocessing data...\n",
      "\n",
      "2. Training Word2Vec embeddings...\n",
      "Vocabulary size: 27815\n",
      "\n",
      "3. Encoding sequences...\n",
      "Label classes: ['NO' 'OO' 'OR' 'OS']\n",
      "\n",
      "4. Starting 5-Fold Cross-Validation...\n",
      "======================================================================\n",
      "\n",
      "============================================================\n",
      " Fold 1\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5 | Train F1: 0.2760 | Val F1: 0.2814 | Gap: -0.0053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 608\u001b[0m\n\u001b[0;32m    604\u001b[0m test_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:/major project/data/test.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 608\u001b[0m model, results \u001b[38;5;241m=\u001b[39m train_with_cross_validation(train_df, test_df, n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m✓ GRU with Cross-Validation training complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    611\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCV Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcv_mean_f1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ± \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcv_std_f1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 461\u001b[0m, in \u001b[0;36mtrain_with_cross_validation\u001b[1;34m(train_df, test_df, n_splits, save_dir)\u001b[0m\n\u001b[0;32m    458\u001b[0m all_histories \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold_num, (train_idx, val_idx) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(skf\u001b[38;5;241m.\u001b[39msplit(train_df, train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabel_Multiclass\u001b[39m\u001b[38;5;124m'\u001b[39m])):\n\u001b[1;32m--> 461\u001b[0m     best_val_f1, fold_history \u001b[38;5;241m=\u001b[39m train_single_fold(\n\u001b[0;32m    462\u001b[0m         train_idx, val_idx, train_df, embedding_matrix, vocab, le,\n\u001b[0;32m    463\u001b[0m         device, fold_num, save_dir\n\u001b[0;32m    464\u001b[0m     )\n\u001b[0;32m    465\u001b[0m     fold_scores\u001b[38;5;241m.\u001b[39mappend(best_val_f1)\n\u001b[0;32m    466\u001b[0m     all_histories\u001b[38;5;241m.\u001b[39mappend(fold_history)\n",
      "Cell \u001b[1;32mIn[3], line 268\u001b[0m, in \u001b[0;36mtrain_single_fold\u001b[1;34m(train_idx, val_idx, train_df, embedding_matrix, vocab, le, device, fold_num, save_dir)\u001b[0m\n\u001b[0;32m    260\u001b[0m fold_history \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: [],\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: [],\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_f1\u001b[39m\u001b[38;5;124m'\u001b[39m: [],\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_f1\u001b[39m\u001b[38;5;124m'\u001b[39m: []\n\u001b[0;32m    265\u001b[0m }\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m--> 268\u001b[0m     train_loss, train_f1 \u001b[38;5;241m=\u001b[39m train_epoch(\n\u001b[0;32m    269\u001b[0m         model, train_loader, optimizer, device, class_weights_tensor\n\u001b[0;32m    270\u001b[0m     )\n\u001b[0;32m    272\u001b[0m     val_loss, val_f1, _, _ \u001b[38;5;241m=\u001b[39m evaluate(\n\u001b[0;32m    273\u001b[0m         model, val_loader, device, class_weights_tensor\n\u001b[0;32m    274\u001b[0m     )\n\u001b[0;32m    276\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[1;32mIn[3], line 122\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, dataloader, optimizer, device, class_weights)\u001b[0m\n\u001b[0;32m    118\u001b[0m all_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    120\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(weight\u001b[38;5;241m=\u001b[39mclass_weights)\n\u001b[1;32m--> 122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m'\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    123\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    124\u001b[0m     labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:734\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 734\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    740\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:790\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    789\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 790\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    791\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    792\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[3], line 48\u001b[0m, in \u001b[0;36mHateSpeechDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     43\u001b[0m     mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandom(\u001b[38;5;28mlen\u001b[39m(input_ids)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m     44\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m [t \u001b[38;5;28;01mif\u001b[39;00m m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(input_ids, mask)]\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(input_ids, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong),\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m     49\u001b[0m }\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# \"\"\"\n",
    "# FINAL: GRU Model with K-Fold Cross-Validation\n",
    "# Best practices: CV for robust evaluation, reduced patience, ensemble predictions\n",
    "# \"\"\"\n",
    "# import os\n",
    "# import sys\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from gensim.models import Word2Vec\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "# from sklearn.metrics import f1_score\n",
    "# from tqdm import tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "# import random\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# # sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "# sys.path.append(os.path.abspath(\"..\"))\n",
    "# from utils.preprocessing import preprocess_for_ml_gru\n",
    "# from utils.evaluation import compute_metrics, print_metrics, plot_confusion_matrix\n",
    "\n",
    "\n",
    "# class HateSpeechDataset(Dataset):\n",
    "#     def __init__(self, input_ids, labels, augment=False):\n",
    "#         self.input_ids = input_ids\n",
    "#         self.labels = labels\n",
    "#         self.augment = augment\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.input_ids)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         input_ids = self.input_ids[idx].copy()\n",
    "        \n",
    "#         if self.augment and random.random() < 0.15:\n",
    "#             mask = np.random.random(len(input_ids)) > 0.1\n",
    "#             input_ids = [t if m else 0 for t, m in zip(input_ids, mask)]\n",
    "        \n",
    "#         return {\n",
    "#             'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "#             'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "#         }\n",
    "\n",
    "\n",
    "# class OptimizedGRUClassifier(nn.Module):\n",
    "#     \"\"\"\n",
    "#     FINAL OPTIMIZED VERSION:\n",
    "#     - Single GRU layer (simpler)\n",
    "#     - Strong dropout (0.5)\n",
    "#     - Smaller hidden size (96)\n",
    "#     - No complex attention (prevents overfitting)\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def __init__(self, embedding_matrix, hidden_dim=96, output_dim=4, dropout=0.5):\n",
    "#         super(OptimizedGRUClassifier, self).__init__()\n",
    "        \n",
    "#         num_embeddings, embedding_dim = embedding_matrix.shape\n",
    "        \n",
    "#         self.embedding = nn.Embedding.from_pretrained(\n",
    "#             torch.FloatTensor(embedding_matrix), \n",
    "#             freeze=False\n",
    "#         )\n",
    "#         self.embedding_dropout = nn.Dropout(0.3)\n",
    "        \n",
    "#         # Single bidirectional GRU\n",
    "#         self.gru = nn.GRU(\n",
    "#             embedding_dim, \n",
    "#             hidden_dim,\n",
    "#             num_layers=1,\n",
    "#             batch_first=True,\n",
    "#             bidirectional=True\n",
    "#         )\n",
    "        \n",
    "#         # Strong regularization\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "#         # Simple classifier\n",
    "#         self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.dropout2 = nn.Dropout(dropout)\n",
    "#         self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         embedded = self.embedding_dropout(self.embedding(x))\n",
    "#         gru_output, hidden = self.gru(embedded)\n",
    "        \n",
    "#         # Concatenate last hidden states\n",
    "#         hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "#         hidden = self.dropout(hidden)\n",
    "        \n",
    "#         # Classifier\n",
    "#         out = self.fc1(hidden)\n",
    "#         out = self.relu(out)\n",
    "#         out = self.dropout2(out)\n",
    "#         logits = self.fc2(out)\n",
    "        \n",
    "#         return logits\n",
    "\n",
    "\n",
    "# def encode_and_pad(tokens, word2idx, max_len=40):\n",
    "#     indices = [word2idx.get(tok, 0) for tok in tokens[:max_len]]\n",
    "#     if len(indices) < max_len:\n",
    "#         indices += [0] * (max_len - len(indices))\n",
    "#     return indices\n",
    "\n",
    "\n",
    "# def train_epoch(model, dataloader, optimizer, device, class_weights):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     all_preds = []\n",
    "#     all_labels = []\n",
    "    \n",
    "#     criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    \n",
    "#     for batch in tqdm(dataloader, desc='Training', leave=False):\n",
    "#         input_ids = batch['input_ids'].to(device)\n",
    "#         labels = batch['labels'].to(device)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         logits = model(input_ids)\n",
    "#         loss = criterion(logits, labels)\n",
    "        \n",
    "#         loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         total_loss += loss.item()\n",
    "#         preds = torch.argmax(logits, dim=1)\n",
    "#         all_preds.extend(preds.cpu().numpy())\n",
    "#         all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "#     avg_loss = total_loss / len(dataloader)\n",
    "#     f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    \n",
    "#     return avg_loss, f1\n",
    "\n",
    "\n",
    "# def evaluate(model, dataloader, device, class_weights):\n",
    "#     model.eval()\n",
    "#     total_loss = 0\n",
    "#     all_preds = []\n",
    "#     all_labels = []\n",
    "    \n",
    "#     criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for batch in tqdm(dataloader, desc='Evaluating', leave=False):\n",
    "#             input_ids = batch['input_ids'].to(device)\n",
    "#             labels = batch['labels'].to(device)\n",
    "            \n",
    "#             logits = model(input_ids)\n",
    "#             loss = criterion(logits, labels)\n",
    "            \n",
    "#             total_loss += loss.item()\n",
    "#             preds = torch.argmax(logits, dim=1)\n",
    "#             all_preds.extend(preds.cpu().numpy())\n",
    "#             all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "#     avg_loss = total_loss / len(dataloader)\n",
    "#     f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    \n",
    "#     return avg_loss, f1, all_preds, all_labels\n",
    "\n",
    "\n",
    "# def plot_fold_history(history, fold_num, save_dir):\n",
    "#     \"\"\"Plot training history for a single fold\"\"\"\n",
    "#     import matplotlib.pyplot as plt\n",
    "    \n",
    "#     fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "#     # Loss plot\n",
    "#     axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2, color='#1f77b4')\n",
    "#     axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2, color='#ff7f0e')\n",
    "#     axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "#     axes[0].set_ylabel('Loss', fontsize=12)\n",
    "#     axes[0].set_title(f'Fold {fold_num + 1} - Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "#     axes[0].legend(fontsize=10)\n",
    "#     axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "#     # F1 plot (as accuracy proxy)\n",
    "#     axes[1].plot(history['train_f1'], label='Train F1', linewidth=2, color='#1f77b4')\n",
    "#     axes[1].plot(history['val_f1'], label='Val F1', linewidth=2, color='#ff7f0e')\n",
    "#     axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "#     axes[1].set_ylabel('F1 Score', fontsize=12)\n",
    "#     axes[1].set_title(f'Fold {fold_num + 1} - Training and Validation F1', fontsize=14, fontweight='bold')\n",
    "#     axes[1].legend(fontsize=10)\n",
    "#     axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(os.path.join(save_dir, f'fold_{fold_num + 1}_history.png'), dpi=300, bbox_inches='tight')\n",
    "#     plt.close()\n",
    "    \n",
    "#     print(f\"✓ Fold {fold_num + 1} training curves saved\")\n",
    "\n",
    "\n",
    "# def train_single_fold(train_idx, val_idx, train_df, embedding_matrix, vocab, le, \n",
    "#                       device, fold_num, save_dir):\n",
    "#     \"\"\"Train model on a single fold\"\"\"\n",
    "    \n",
    "#     print(f\"\\n{'='*60}\")\n",
    "#     print(f\" Fold {fold_num + 1}\")\n",
    "#     print(f\"{'='*60}\")\n",
    "    \n",
    "#     # Split data\n",
    "#     fold_train_df = train_df.iloc[train_idx].copy()\n",
    "#     fold_val_df = train_df.iloc[val_idx].copy()\n",
    "    \n",
    "#     # Prepare datasets\n",
    "#     y_train = le.transform(fold_train_df['Label_Multiclass'])\n",
    "#     y_val = le.transform(fold_val_df['Label_Multiclass'])\n",
    "    \n",
    "#     train_dataset = HateSpeechDataset(\n",
    "#         fold_train_df['input_ids'].tolist(), \n",
    "#         y_train, \n",
    "#         augment=True\n",
    "#     )\n",
    "#     val_dataset = HateSpeechDataset(fold_val_df['input_ids'].tolist(), y_val)\n",
    "    \n",
    "#     train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=128)\n",
    "    \n",
    "#     # Class weights\n",
    "#     class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "#     class_weights = np.clip(class_weights, 0.5, 4.0)\n",
    "#     class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "    \n",
    "#     # Initialize model\n",
    "#     model = OptimizedGRUClassifier(\n",
    "#         embedding_matrix=embedding_matrix,\n",
    "#         hidden_dim=96,\n",
    "#         output_dim=len(le.classes_),\n",
    "#         dropout=0.5\n",
    "#     ).to(device)\n",
    "    \n",
    "#     # Optimizer with strong regularization\n",
    "#     optimizer = optim.AdamW(\n",
    "#         model.parameters(), \n",
    "#         lr=5e-4,\n",
    "#         weight_decay=1e-3\n",
    "#     )\n",
    "    \n",
    "#     # Cosine annealing\n",
    "#     scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "#         optimizer, T_0=5, T_mult=2\n",
    "#     )\n",
    "    \n",
    "#     # Training loop\n",
    "#     epochs = 50\n",
    "#     best_val_f1 = 0\n",
    "#     patience = 5  # REDUCED from 8\n",
    "#     patience_counter = 0\n",
    "    \n",
    "#     fold_history = {\n",
    "#         'train_loss': [],\n",
    "#         'val_loss': [],\n",
    "#         'train_f1': [],\n",
    "#         'val_f1': []\n",
    "#     }\n",
    "    \n",
    "#     for epoch in range(epochs):\n",
    "#         train_loss, train_f1 = train_epoch(\n",
    "#             model, train_loader, optimizer, device, class_weights_tensor\n",
    "#         )\n",
    "        \n",
    "#         val_loss, val_f1, _, _ = evaluate(\n",
    "#             model, val_loader, device, class_weights_tensor\n",
    "#         )\n",
    "        \n",
    "#         scheduler.step()\n",
    "        \n",
    "#         fold_history['train_loss'].append(train_loss)\n",
    "#         fold_history['val_loss'].append(val_loss)\n",
    "#         fold_history['train_f1'].append(train_f1)\n",
    "#         fold_history['val_f1'].append(val_f1)\n",
    "        \n",
    "#         f1_gap = train_f1 - val_f1\n",
    "        \n",
    "#         if (epoch + 1) % 5 == 0:\n",
    "#             print(f\"Epoch {epoch + 1:2d} | Train F1: {train_f1:.4f} | Val F1: {val_f1:.4f} | Gap: {f1_gap:.4f}\")\n",
    "        \n",
    "#         if val_f1 > best_val_f1:\n",
    "#             best_val_f1 = val_f1\n",
    "#             patience_counter = 0\n",
    "            \n",
    "#             # Save best model for this fold\n",
    "#             torch.save({\n",
    "#                 'model_state_dict': model.state_dict(),\n",
    "#                 'val_f1': val_f1,\n",
    "#                 'fold': fold_num\n",
    "#             }, os.path.join(save_dir, f'gru_fold_{fold_num}.pt'))\n",
    "#         else:\n",
    "#             patience_counter += 1\n",
    "        \n",
    "#         if patience_counter >= patience:\n",
    "#             print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "#             break\n",
    "    \n",
    "#     print(f\"Fold {fold_num + 1} Best Val F1: {best_val_f1:.4f}\")\n",
    "    \n",
    "#     # Plot this fold's history\n",
    "#     plot_fold_history(fold_history, fold_num, save_dir)\n",
    "    \n",
    "#     return best_val_f1, fold_history\n",
    "\n",
    "# def plot_cv_summary(fold_scores, all_histories, save_dir):\n",
    "#     \"\"\"\n",
    "#     Summarizes K-Fold cross-validation results:\n",
    "#       - Bar chart for each fold's validation F1-score\n",
    "#       - Line chart for averaged loss/F1 curves across folds\n",
    "#     \"\"\"\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     import numpy as np\n",
    "#     import os\n",
    "\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "#     # --- 1️⃣ Fold F1 Score Summary (Bar Chart) ---\n",
    "#     plt.figure(figsize=(8, 5))\n",
    "#     plt.bar(\n",
    "#         range(1, len(fold_scores) + 1),\n",
    "#         fold_scores,\n",
    "#         color=\"#1f77b4\",\n",
    "#         alpha=0.8\n",
    "#     )\n",
    "#     plt.axhline(np.mean(fold_scores), color=\"red\", linestyle=\"--\", label=\"Mean F1\")\n",
    "#     plt.xlabel(\"Fold\", fontsize=12)\n",
    "#     plt.ylabel(\"Validation F1 Score\", fontsize=12)\n",
    "#     plt.title(\"Cross-Validation F1 Scores per Fold\", fontsize=14, fontweight=\"bold\")\n",
    "#     plt.xticks(range(1, len(fold_scores) + 1))\n",
    "#     plt.legend(fontsize=10)\n",
    "#     plt.grid(alpha=0.3)\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(os.path.join(save_dir, \"cv_f1_summary.png\"), dpi=300)\n",
    "#     plt.close()\n",
    "\n",
    "#     # --- 2️⃣ Average Training Curves Across Folds ---\n",
    "#     max_epochs = max(len(h[\"train_loss\"]) for h in all_histories)\n",
    "\n",
    "#     # Normalize histories to same length (padding with NaN for shorter folds)\n",
    "#     train_losses = np.array([\n",
    "#         np.pad(h[\"train_loss\"], (0, max_epochs - len(h[\"train_loss\"])), constant_values=np.nan)\n",
    "#         for h in all_histories\n",
    "#     ])\n",
    "#     val_losses = np.array([\n",
    "#         np.pad(h[\"val_loss\"], (0, max_epochs - len(h[\"val_loss\"])), constant_values=np.nan)\n",
    "#         for h in all_histories\n",
    "#     ])\n",
    "#     train_f1s = np.array([\n",
    "#         np.pad(h[\"train_f1\"], (0, max_epochs - len(h[\"train_f1\"])), constant_values=np.nan)\n",
    "#         for h in all_histories\n",
    "#     ])\n",
    "#     val_f1s = np.array([\n",
    "#         np.pad(h[\"val_f1\"], (0, max_epochs - len(h[\"val_f1\"])), constant_values=np.nan)\n",
    "#         for h in all_histories\n",
    "#     ])\n",
    "\n",
    "#     # Compute mean (ignoring NaN from early-stopped folds)\n",
    "#     mean_train_loss = np.nanmean(train_losses, axis=0)\n",
    "#     mean_val_loss = np.nanmean(val_losses, axis=0)\n",
    "#     mean_train_f1 = np.nanmean(train_f1s, axis=0)\n",
    "#     mean_val_f1 = np.nanmean(val_f1s, axis=0)\n",
    "\n",
    "#     # --- Plot averaged curves ---\n",
    "#     fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "#     # Loss\n",
    "#     axes[0].plot(mean_train_loss, label=\"Train Loss\", linewidth=2, color=\"#1f77b4\")\n",
    "#     axes[0].plot(mean_val_loss, label=\"Val Loss\", linewidth=2, color=\"#ff7f0e\")\n",
    "#     axes[0].set_title(\"Average Training & Validation Loss\", fontsize=14, fontweight=\"bold\")\n",
    "#     axes[0].set_xlabel(\"Epoch\")\n",
    "#     axes[0].set_ylabel(\"Loss\")\n",
    "#     axes[0].grid(alpha=0.3)\n",
    "#     axes[0].legend(fontsize=10)\n",
    "\n",
    "#     # F1\n",
    "#     axes[1].plot(mean_train_f1, label=\"Train F1\", linewidth=2, color=\"#1f77b4\")\n",
    "#     axes[1].plot(mean_val_f1, label=\"Val F1\", linewidth=2, color=\"#ff7f0e\")\n",
    "#     axes[1].set_title(\"Average Training & Validation F1\", fontsize=14, fontweight=\"bold\")\n",
    "#     axes[1].set_xlabel(\"Epoch\")\n",
    "#     axes[1].set_ylabel(\"F1 Score\")\n",
    "#     axes[1].grid(alpha=0.3)\n",
    "#     axes[1].legend(fontsize=10)\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(os.path.join(save_dir, \"cv_training_summary.png\"), dpi=300)\n",
    "#     plt.close()\n",
    "\n",
    "#     print(\"✓ Saved Cross-Validation summary plots: `cv_f1_summary.png` and `cv_training_summary.png`\")\n",
    "\n",
    "# def plot_final_training_history(final_history, save_dir):\n",
    "#     \"\"\"\n",
    "#     Plot the final GRU model's training history after full-data training.\n",
    "\n",
    "#     Args:\n",
    "#         final_history (dict): Dictionary containing 'train_loss' and 'train_f1' lists.\n",
    "#         save_dir (str): Directory to save plots.\n",
    "#     \"\"\"\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     import os\n",
    "\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "#     epochs = range(1, len(final_history['train_loss']) + 1)\n",
    "\n",
    "#     fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "#     # --------- Loss Curve ---------\n",
    "#     axes[0].plot(epochs, final_history['train_loss'], label='Train Loss', color='#1f77b4', linewidth=2)\n",
    "#     axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "#     axes[0].set_ylabel('Loss', fontsize=12)\n",
    "#     axes[0].set_title('Final Model - Training Loss', fontsize=14, fontweight='bold')\n",
    "#     axes[0].grid(alpha=0.3)\n",
    "#     axes[0].legend(fontsize=10)\n",
    "\n",
    "#     # --------- F1 Curve ---------\n",
    "#     axes[1].plot(epochs, final_history['train_f1'], label='Train F1', color='#ff7f0e', linewidth=2)\n",
    "#     axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "#     axes[1].set_ylabel('F1 Score', fontsize=12)\n",
    "#     axes[1].set_title('Final Model - Training F1 Score', fontsize=14, fontweight='bold')\n",
    "#     axes[1].grid(alpha=0.3)\n",
    "#     axes[1].legend(fontsize=10)\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(os.path.join(save_dir, \"gru_final_training_history.png\"), dpi=300, bbox_inches='tight')\n",
    "#     plt.close()\n",
    "\n",
    "#     print(\"✓ Saved final training history plot as `gru_final_training_history.png`\")\n",
    "\n",
    "\n",
    "# def train_with_cross_validation(train_df, test_df, n_splits=5, save_dir='models/saved_models'):\n",
    "#     \"\"\"Train GRU with K-Fold Cross-Validation\"\"\"\n",
    "    \n",
    "#     print(\"\\n\" + \"=\"*70)\n",
    "#     print(\" GRU MODEL WITH 5-FOLD CROSS-VALIDATION\")\n",
    "#     print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     print(f\"Using device: {device}\")\n",
    "    \n",
    "#     # Preprocess\n",
    "#     print(\"\\n1. Preprocessing data...\")\n",
    "#     for df in [train_df, test_df]:\n",
    "#         if 'clean_comment' not in df.columns:\n",
    "#             df['clean_comment'] = df['Comment'].apply(preprocess_for_ml_gru)\n",
    "#         if 'tokens' not in df.columns:\n",
    "#             df['tokens'] = df['clean_comment'].apply(str.split)\n",
    "    \n",
    "#     # Build Word2Vec\n",
    "#     print(\"\\n2. Training Word2Vec embeddings...\")\n",
    "#     all_tokens = train_df['tokens'].tolist()\n",
    "#     w2v_model = Word2Vec(\n",
    "#         sentences=all_tokens,\n",
    "#         vector_size=100,\n",
    "#         window=5,\n",
    "#         min_count=1,\n",
    "#         workers=4,\n",
    "#         epochs=10,\n",
    "#         sg=1\n",
    "#     )\n",
    "    \n",
    "#     vocab = {word: idx+1 for idx, word in enumerate(w2v_model.wv.index_to_key)}\n",
    "#     vocab['<UNK>'] = 0\n",
    "#     vocab['<PAD>'] = 0\n",
    "    \n",
    "#     embedding_matrix = np.zeros((len(vocab), 100))\n",
    "#     for word, idx in vocab.items():\n",
    "#         if word in w2v_model.wv:\n",
    "#             embedding_matrix[idx] = w2v_model.wv[word]\n",
    "    \n",
    "#     print(f\"Vocabulary size: {len(vocab)}\")\n",
    "    \n",
    "#     # Encode sequences\n",
    "#     print(\"\\n3. Encoding sequences...\")\n",
    "#     for df in [train_df, test_df]:\n",
    "#         df['input_ids'] = df['tokens'].apply(lambda x: encode_and_pad(x, vocab, 40))\n",
    "    \n",
    "#     # Labels\n",
    "#     le = LabelEncoder()\n",
    "#     le.fit(train_df['Label_Multiclass'])\n",
    "#     print(f\"Label classes: {le.classes_}\")\n",
    "    \n",
    "#     # Cross-validation\n",
    "#     print(f\"\\n4. Starting {n_splits}-Fold Cross-Validation...\")\n",
    "#     print(\"=\"*70)\n",
    "    \n",
    "#     skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "#     fold_scores = []\n",
    "#     all_histories = []\n",
    "    \n",
    "#     for fold_num, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['Label_Multiclass'])):\n",
    "#         best_val_f1, fold_history = train_single_fold(\n",
    "#             train_idx, val_idx, train_df, embedding_matrix, vocab, le,\n",
    "#             device, fold_num, save_dir\n",
    "#         )\n",
    "#         fold_scores.append(best_val_f1)\n",
    "#         all_histories.append(fold_history)\n",
    "    \n",
    "#     # Cross-validation results\n",
    "#     print(\"\\n\" + \"=\"*70)\n",
    "#     print(\" CROSS-VALIDATION RESULTS\")\n",
    "#     print(\"=\"*70)\n",
    "    \n",
    "#     for i, score in enumerate(fold_scores):\n",
    "#         print(f\"Fold {i+1}: {score:.4f}\")\n",
    "    \n",
    "#     mean_f1 = np.mean(fold_scores)\n",
    "#     std_f1 = np.std(fold_scores)\n",
    "    \n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(f\"Mean Val F1: {mean_f1:.4f} ± {std_f1:.4f}\")\n",
    "#     print(f\"{'='*70}\")\n",
    "    \n",
    "#     # Plot CV summary\n",
    "#     plot_cv_summary(fold_scores, all_histories, save_dir)\n",
    "    \n",
    "#     # Train final model on ALL training data\n",
    "#     print(\"\\n5. Training final model on full training set...\")\n",
    "    \n",
    "#     y_train_full = le.transform(train_df['Label_Multiclass'])\n",
    "#     y_test = le.transform(test_df['Label_Multiclass'])\n",
    "    \n",
    "#     train_dataset_full = HateSpeechDataset(\n",
    "#         train_df['input_ids'].tolist(), \n",
    "#         y_train_full, \n",
    "#         augment=True\n",
    "#     )\n",
    "#     test_dataset = HateSpeechDataset(test_df['input_ids'].tolist(), y_test)\n",
    "    \n",
    "#     train_loader_full = DataLoader(train_dataset_full, batch_size=128, shuffle=True)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=128)\n",
    "    \n",
    "#     # Class weights\n",
    "#     class_weights = compute_class_weight('balanced', classes=np.unique(y_train_full), y=y_train_full)\n",
    "#     class_weights = np.clip(class_weights, 0.5, 4.0)\n",
    "#     class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "    \n",
    "#     # Final model\n",
    "#     final_model = OptimizedGRUClassifier(\n",
    "#         embedding_matrix=embedding_matrix,\n",
    "#         hidden_dim=96,\n",
    "#         output_dim=len(le.classes_),\n",
    "#         dropout=0.5\n",
    "#     ).to(device)\n",
    "    \n",
    "#     optimizer = optim.AdamW(final_model.parameters(), lr=5e-4, weight_decay=1e-3)\n",
    "#     scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5)\n",
    "    \n",
    "#     # Train final model (fewer epochs based on CV)\n",
    "#     best_epochs = int(np.mean([len(h['train_loss']) for h in all_histories]))\n",
    "#     print(f\"Training for {best_epochs} epochs (based on CV average)...\")\n",
    "    \n",
    "#     final_history = {\n",
    "#         'train_loss': [],\n",
    "#         'train_f1': []\n",
    "#     }\n",
    "    \n",
    "#     for epoch in range(best_epochs):\n",
    "#         train_loss, train_f1 = train_epoch(\n",
    "#             final_model, train_loader_full, optimizer, device, class_weights_tensor\n",
    "#         )\n",
    "#         scheduler.step()\n",
    "        \n",
    "#         final_history['train_loss'].append(train_loss)\n",
    "#         final_history['train_f1'].append(train_f1)\n",
    "        \n",
    "#         if (epoch + 1) % 5 == 0:\n",
    "#             print(f\"Epoch {epoch + 1}/{best_epochs} | Train Loss: {train_loss:.4f} | Train F1: {train_f1:.4f}\")\n",
    "    \n",
    "#     # Plot final model history\n",
    "#     plot_final_training_history(final_history, save_dir)\n",
    "    \n",
    "#     # Save final model\n",
    "#     torch.save({\n",
    "#         'model_state_dict': final_model.state_dict(),\n",
    "#         'vocab': vocab,\n",
    "#         'label_encoder': le,\n",
    "#         'cv_mean_f1': mean_f1,\n",
    "#         'cv_std_f1': std_f1\n",
    "#     }, os.path.join(save_dir, 'gru_final_cv_model.pt'))\n",
    "    \n",
    "#     # Evaluate on test set\n",
    "#     print(\"\\n6. Evaluating on test set...\")\n",
    "#     _, test_f1, test_preds, test_labels = evaluate(\n",
    "#         final_model, test_loader, device, class_weights_tensor\n",
    "#     )\n",
    "    \n",
    "#     test_metrics = compute_metrics(test_labels, test_preds, labels=le.classes_)\n",
    "    \n",
    "#     print(\"\\n\" + \"=\"*70)\n",
    "#     print_metrics(test_metrics, title=\"GRU with Cross-Validation - Test Results\")\n",
    "#     print(\"=\"*70)\n",
    "    \n",
    "#     plot_confusion_matrix(\n",
    "#         test_labels, test_preds,\n",
    "#         labels=le.classes_,\n",
    "#         save_path=os.path.join(save_dir, 'gru_cv_confusion_matrix.png'),\n",
    "#         title=\"GRU with CV - Confusion Matrix\"\n",
    "#     )\n",
    "    \n",
    "#     # Summary\n",
    "#     results = {\n",
    "#         'model': 'GRU with 5-Fold CV',\n",
    "#         'cv_mean_f1': mean_f1,\n",
    "#         'cv_std_f1': std_f1,\n",
    "#         'test_f1': test_f1,\n",
    "#         'fold_scores': fold_scores,\n",
    "#         'test_metrics': test_metrics\n",
    "#     }\n",
    "    \n",
    "#     print(\"\\n\" + \"=\"*70)\n",
    "#     print(\" FINAL SUMMARY\")\n",
    "#     print(\"=\"*70)\n",
    "#     print(f\"Cross-Validation: {mean_f1:.4f} ± {std_f1:.4f}\")\n",
    "#     print(f\"Test Set F1:      {test_f1:.4f}\")\n",
    "#     print(f\"\\n✓ Model is {'well-generalized' if abs(mean_f1 - test_f1) < 0.05 else 'showing some variance'}\")\n",
    "#     print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "#     # Save results\n",
    "#     import json\n",
    "#     with open(os.path.join(save_dir, 'gru_cv_results.json'), 'w') as f:\n",
    "#         json.dump({\n",
    "#             'cv_mean_f1': float(mean_f1),\n",
    "#             'cv_std_f1': float(std_f1),\n",
    "#             'test_f1': float(test_f1),\n",
    "#             'fold_scores': [float(s) for s in fold_scores]\n",
    "#         }, f, indent=2)\n",
    "    \n",
    "#     return final_model, results\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     print(\"Loading data...\")\n",
    "#     train_df = pd.read_json(\"D:/major project/data/train.json\")\n",
    "#     test_df = pd.read_json(\"D:/major project/data/test.json\")\n",
    "    \n",
    "#     print(f\"Train: {len(train_df)}, Test: {len(test_df)}\")\n",
    "    \n",
    "#     model, results = train_with_cross_validation(train_df, test_df, n_splits=5)\n",
    "    \n",
    "#     print(\"\\n✓ GRU with Cross-Validation training complete!\")\n",
    "#     print(f\"CV Score: {results['cv_mean_f1']:.4f} ± {results['cv_std_f1']:.4f}\")\n",
    "#     print(f\"Test F1:  {results['test_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dc15dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FINAL: GRU Model with K-Fold Cross-Validation\n",
    "IMPROVEMENTS:\n",
    "- Per-fold training plots ✓\n",
    "- Confusion matrices for validation and test ✓\n",
    "- Complete visualization suite ✓\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Dataset Definition\n",
    "# -------------------------------------------------------------\n",
    "class HateSpeechDataset(Dataset):\n",
    "    def __init__(self, input_ids, labels, augment=False):\n",
    "        self.input_ids = input_ids\n",
    "        self.labels = labels\n",
    "        self.augment = augment\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.input_ids[idx].copy()\n",
    "        if self.augment and random.random() < 0.15:\n",
    "            mask = np.random.random(len(input_ids)) > 0.1\n",
    "            input_ids = [t if m else 0 for t, m in zip(input_ids, mask)]\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# GRU Classifier\n",
    "# -------------------------------------------------------------\n",
    "class OptimizedGRUClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim=96, output_dim=4, dropout=0.5):\n",
    "        super(OptimizedGRUClassifier, self).__init__()\n",
    "        num_embeddings, embedding_dim = embedding_matrix.shape\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.FloatTensor(embedding_matrix), freeze=False\n",
    "        )\n",
    "        self.embedding_dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            embedding_dim, hidden_dim, num_layers=1, batch_first=True, bidirectional=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding_dropout(self.embedding(x))\n",
    "        _, hidden = self.gru(embedded)\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        hidden = self.dropout(hidden)\n",
    "        out = self.fc1(hidden)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout2(out)\n",
    "        logits = self.fc2(out)\n",
    "        return logits\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Utilities\n",
    "# -------------------------------------------------------------\n",
    "def encode_and_pad(tokens, word2idx, max_len=40):\n",
    "    indices = [word2idx.get(tok, 0) for tok in tokens[:max_len]]\n",
    "    if len(indices) < max_len:\n",
    "        indices += [0] * (max_len - len(indices))\n",
    "    return indices\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, device, class_weights):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    total_loss, all_preds, all_labels = 0, [], []\n",
    "    for batch in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    return avg_loss, f1\n",
    "\n",
    "def evaluate(model, dataloader, device, class_weights):\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    total_loss, all_preds, all_labels = 0, [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            logits = model(input_ids)\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    return avg_loss, f1, all_preds, all_labels\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Visualization Functions\n",
    "# -------------------------------------------------------------\n",
    "def plot_confusion_matrix_custom(y_true, y_pred, labels, save_path, title=\"Confusion Matrix\"):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues',\n",
    "                xticklabels=labels, yticklabels=labels,\n",
    "                cbar_kws={'label': 'Proportion'})\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"✓ Confusion matrix saved: {os.path.basename(save_path)}\")\n",
    "\n",
    "\n",
    "def plot_fold_history(history, fold_num, save_dir):\n",
    "    \"\"\"Plot training curves for a single fold\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    # Loss plot\n",
    "    axes[0].plot(epochs, history['train_loss'], label='Train Loss', \n",
    "                linewidth=2, color='#1f77b4', marker='o', markersize=3)\n",
    "    axes[0].plot(epochs, history['val_loss'], label='Val Loss', \n",
    "                linewidth=2, color='#ff7f0e', marker='s', markersize=3)\n",
    "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0].set_ylabel('Loss', fontsize=12)\n",
    "    axes[0].set_title(f'Fold {fold_num + 1} - Training and Validation Loss', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "    axes[0].legend(fontsize=10)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # F1 plot\n",
    "    axes[1].plot(epochs, history['train_f1'], label='Train F1', \n",
    "                linewidth=2, color='#1f77b4', marker='o', markersize=3)\n",
    "    axes[1].plot(epochs, history['val_f1'], label='Val F1', \n",
    "                linewidth=2, color='#ff7f0e', marker='s', markersize=3)\n",
    "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1].set_ylabel('F1 Score', fontsize=12)\n",
    "    axes[1].set_title(f'Fold {fold_num + 1} - Training and Validation F1', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "    axes[1].legend(fontsize=10)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(save_dir, f'fold_{fold_num + 1}_history.png')\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"✓ Fold {fold_num + 1} training curves saved\")\n",
    "\n",
    "\n",
    "def plot_cv_summary(fold_scores, all_histories, save_dir):\n",
    "    \"\"\"Plots K-Fold summary charts\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Bar chart of fold scores\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(range(1, len(fold_scores) + 1), fold_scores, color=\"#1f77b4\", edgecolor='navy')\n",
    "    plt.axhline(np.mean(fold_scores), color=\"red\", linestyle=\"--\", linewidth=2,\n",
    "                label=f\"Mean: {np.mean(fold_scores):.4f}\")\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, score) in enumerate(zip(bars, fold_scores)):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, score + 0.01, \n",
    "                f'{score:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.xlabel(\"Fold\", fontsize=12)\n",
    "    plt.ylabel(\"Validation F1\", fontsize=12)\n",
    "    plt.title(\"Cross-Validation: Best F1 Score per Fold\", fontsize=14, fontweight=\"bold\")\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, \"cv_f1_summary.png\"), dpi=300)\n",
    "    plt.close()\n",
    "    print(\"✓ CV summary (F1 bar chart) saved\")\n",
    "\n",
    "    # Mean curves across folds\n",
    "    max_epochs = max(len(h[\"train_loss\"]) for h in all_histories)\n",
    "    \n",
    "    def pad_history(hist_list, key):\n",
    "        padded = []\n",
    "        for h in hist_list:\n",
    "            arr = np.array(h[key])\n",
    "            if len(arr) < max_epochs:\n",
    "                arr = np.pad(arr, (0, max_epochs - len(arr)), constant_values=np.nan)\n",
    "            padded.append(arr)\n",
    "        return np.array(padded)\n",
    "    \n",
    "    mean_train_loss = np.nanmean(pad_history(all_histories, \"train_loss\"), axis=0)\n",
    "    mean_val_loss = np.nanmean(pad_history(all_histories, \"val_loss\"), axis=0)\n",
    "    mean_train_f1 = np.nanmean(pad_history(all_histories, \"train_f1\"), axis=0)\n",
    "    mean_val_f1 = np.nanmean(pad_history(all_histories, \"val_f1\"), axis=0)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    epochs = range(1, max_epochs + 1)\n",
    "    \n",
    "    # Average loss\n",
    "    axes[0].plot(epochs, mean_train_loss, label=\"Train Loss\", linewidth=2, color='#1f77b4')\n",
    "    axes[0].plot(epochs, mean_val_loss, label=\"Val Loss\", linewidth=2, color='#ff7f0e')\n",
    "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0].set_ylabel('Loss', fontsize=12)\n",
    "    axes[0].set_title(\"Average Train/Val Loss Across Folds\", fontsize=14, fontweight='bold')\n",
    "    axes[0].legend(fontsize=10)\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Average F1\n",
    "    axes[1].plot(epochs, mean_train_f1, label=\"Train F1\", linewidth=2, color='#1f77b4')\n",
    "    axes[1].plot(epochs, mean_val_f1, label=\"Val F1\", linewidth=2, color='#ff7f0e')\n",
    "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1].set_ylabel('F1 Score', fontsize=12)\n",
    "    axes[1].set_title(\"Average Train/Val F1 Across Folds\", fontsize=14, fontweight='bold')\n",
    "    axes[1].legend(fontsize=10)\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, \"cv_training_summary.png\"), dpi=300)\n",
    "    plt.close()\n",
    "    print(\"✓ CV training summary (average curves) saved\")\n",
    "\n",
    "\n",
    "def plot_final_training_history(final_history, save_dir):\n",
    "    \"\"\"Plot final model train/val loss and F1 curves.\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    epochs = range(1, len(final_history['train_loss']) + 1)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0].plot(epochs, final_history['train_loss'], label=\"Train Loss\", \n",
    "                linewidth=2, color='#1f77b4', marker='o', markersize=4)\n",
    "    axes[0].plot(epochs, final_history['val_loss'], label=\"Val Loss\", \n",
    "                linewidth=2, color='#ff7f0e', marker='s', markersize=4, linestyle='--')\n",
    "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0].set_ylabel('Loss', fontsize=12)\n",
    "    axes[0].set_title(\"Final Model - Training and Validation Loss\", fontsize=14, fontweight='bold')\n",
    "    axes[0].legend(fontsize=10)\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # F1\n",
    "    axes[1].plot(epochs, final_history['train_f1'], label=\"Train F1\", \n",
    "                linewidth=2, color='#2ca02c', marker='o', markersize=4)\n",
    "    axes[1].plot(epochs, final_history['val_f1'], label=\"Val F1\", \n",
    "                linewidth=2, color='#d62728', marker='s', markersize=4, linestyle='--')\n",
    "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1].set_ylabel('F1 Score', fontsize=12)\n",
    "    axes[1].set_title(\"Final Model - Training and Validation F1\", fontsize=14, fontweight='bold')\n",
    "    axes[1].legend(fontsize=10)\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, \"gru_final_training_history.png\"), dpi=300)\n",
    "    plt.close()\n",
    "    print(\"✓ Final model training curves saved\")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# K-Fold Training Function\n",
    "# -------------------------------------------------------------\n",
    "def train_single_fold(train_idx, val_idx, train_df, embedding_matrix, vocab, le, \n",
    "                     device, fold_num, save_dir):\n",
    "    \"\"\"Train one CV fold\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\" Fold {fold_num + 1}/5\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    fold_train_df = train_df.iloc[train_idx].copy()\n",
    "    fold_val_df = train_df.iloc[val_idx].copy()\n",
    "    \n",
    "    y_train = le.transform(fold_train_df['Label_Multiclass'])\n",
    "    y_val = le.transform(fold_val_df['Label_Multiclass'])\n",
    "    \n",
    "    train_ds = HateSpeechDataset(fold_train_df['input_ids'].tolist(), y_train, augment=True)\n",
    "    val_ds = HateSpeechDataset(fold_val_df['input_ids'].tolist(), y_val)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=128)\n",
    "    \n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weights = np.clip(class_weights, 0.5, 4.0)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "    model = OptimizedGRUClassifier(embedding_matrix, hidden_dim=96, \n",
    "                                   output_dim=len(le.classes_), dropout=0.5).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-3)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2)\n",
    "    \n",
    "    best_val_f1, patience, patience_counter = 0, 5, 0\n",
    "    hist = {'train_loss': [], 'val_loss': [], 'train_f1': [], 'val_f1': []}\n",
    "\n",
    "    for epoch in range(50):\n",
    "        tr_loss, tr_f1 = train_epoch(model, train_loader, optimizer, device, class_weights)\n",
    "        val_loss, val_f1, val_preds, val_labels = evaluate(model, val_loader, device, class_weights)\n",
    "        scheduler.step()\n",
    "        \n",
    "        hist['train_loss'].append(tr_loss)\n",
    "        hist['val_loss'].append(val_loss)\n",
    "        hist['train_f1'].append(tr_f1)\n",
    "        hist['val_f1'].append(val_f1)\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1:2d} | Train F1: {tr_f1:.4f} | Val F1: {val_f1:.4f} | Gap: {tr_f1-val_f1:.4f}\")\n",
    "        \n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            patience_counter = 0\n",
    "            best_val_preds = val_preds\n",
    "            best_val_labels = val_labels\n",
    "            torch.save({'model_state_dict': model.state_dict()}, \n",
    "                      os.path.join(save_dir, f'gru_fold_{fold_num}.pt'))\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                break\n",
    "    \n",
    "    print(f\"Fold {fold_num + 1} Best Val F1: {best_val_f1:.4f}\")\n",
    "    \n",
    "    # Plot fold history\n",
    "    plot_fold_history(hist, fold_num, save_dir)\n",
    "    \n",
    "    # Plot fold confusion matrix\n",
    "    cm_path = os.path.join(save_dir, f'fold_{fold_num + 1}_confusion_matrix.png')\n",
    "    plot_confusion_matrix_custom(best_val_labels, best_val_preds, le.classes_, \n",
    "                                cm_path, title=f\"Fold {fold_num + 1} - Validation Confusion Matrix\")\n",
    "    \n",
    "    return best_val_f1, hist\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Main Cross-Validation Training\n",
    "# -------------------------------------------------------------\n",
    "def train_with_cross_validation(train_df, val_df, test_df, n_splits=5, save_dir='models/saved_models'):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\" GRU MODEL WITH 5-FOLD CROSS-VALIDATION\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Using device: {device}\\n\")\n",
    "\n",
    "    # Ensure tokens exist\n",
    "    if 'tokens' not in train_df.columns:\n",
    "        print(\"ERROR: 'tokens' column missing! Run preprocessing first.\")\n",
    "        return\n",
    "\n",
    "    # Word2Vec\n",
    "    print(\"Building Word2Vec embeddings...\")\n",
    "    all_tokens = train_df['tokens'].tolist()\n",
    "    w2v = Word2Vec(sentences=all_tokens, vector_size=100, window=5, \n",
    "                   min_count=1, workers=4, epochs=10, sg=1)\n",
    "    vocab = {w: i+1 for i, w in enumerate(w2v.wv.index_to_key)}\n",
    "    vocab['<PAD>'] = 0\n",
    "    emb_matrix = np.zeros((len(vocab), 100))\n",
    "    for w, i in vocab.items():\n",
    "        if w in w2v.wv:\n",
    "            emb_matrix[i] = w2v.wv[w]\n",
    "    \n",
    "    print(f\"Vocabulary size: {len(vocab)}\")\n",
    "\n",
    "    # Encode sequences\n",
    "    for df in [train_df, val_df, test_df]:\n",
    "        df['input_ids'] = df['tokens'].apply(lambda x: encode_and_pad(x, vocab, 40))\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train_df['Label_Multiclass'])\n",
    "    print(f\"Classes: {le.classes_}\\n\")\n",
    "\n",
    "    # ---- K-Fold Cross-Validation ----\n",
    "    print(\"=\"*70)\n",
    "    print(\" STARTING 5-FOLD CROSS-VALIDATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    fold_scores, histories = [], []\n",
    "    \n",
    "    for fold, (tr_idx, v_idx) in enumerate(skf.split(train_df, train_df['Label_Multiclass'])):\n",
    "        best_f1, hist = train_single_fold(tr_idx, v_idx, train_df, emb_matrix, vocab, \n",
    "                                          le, device, fold, save_dir)\n",
    "        fold_scores.append(best_f1)\n",
    "        histories.append(hist)\n",
    "    \n",
    "    # CV Summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" CROSS-VALIDATION RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    for i, score in enumerate(fold_scores):\n",
    "        print(f\"Fold {i+1}: {score:.4f}\")\n",
    "    \n",
    "    mean_f1, std_f1 = np.mean(fold_scores), np.std(fold_scores)\n",
    "    print(f\"\\nMean Val F1: {mean_f1:.4f} ± {std_f1:.4f}\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    plot_cv_summary(fold_scores, histories, save_dir)\n",
    "\n",
    "    # ---- Final Training with Validation Monitoring ----\n",
    "    print(\"=\"*70)\n",
    "    print(\" TRAINING FINAL MODEL (with validation monitoring)\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    tr_full, val_hold = train_test_split(train_df, test_size=0.1, \n",
    "                                         stratify=train_df['Label_Multiclass'], \n",
    "                                         random_state=42)\n",
    "    y_tr = le.transform(tr_full['Label_Multiclass'])\n",
    "    y_val_hold = le.transform(val_hold['Label_Multiclass'])\n",
    "    \n",
    "    tr_loader = DataLoader(HateSpeechDataset(tr_full['input_ids'].tolist(), y_tr, augment=True), \n",
    "                          batch_size=64, shuffle=True)\n",
    "    val_hold_loader = DataLoader(HateSpeechDataset(val_hold['input_ids'].tolist(), y_val_hold), \n",
    "                                 batch_size=64)\n",
    "    \n",
    "    weights = compute_class_weight('balanced', classes=np.unique(y_tr), y=y_tr)\n",
    "    weights = torch.tensor(weights, dtype=torch.float).to(device)\n",
    "\n",
    "    final_model = OptimizedGRUClassifier(emb_matrix, output_dim=len(le.classes_)).to(device)\n",
    "    opt = optim.AdamW(final_model.parameters(), lr=1e-4, weight_decay=1e-3)\n",
    "    sched = optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', patience=2, factor=0.5)\n",
    "    \n",
    "    best_val, patience, counter = 0, 3, 0\n",
    "    final_hist = {'train_loss': [], 'train_f1': [], 'val_loss': [], 'val_f1': []}\n",
    "\n",
    "    for ep in range(30):\n",
    "        tr_loss, tr_f1 = train_epoch(final_model, tr_loader, opt, device, weights)\n",
    "        val_loss, val_f1, val_preds, val_labels = evaluate(final_model, val_hold_loader, device, weights)\n",
    "        sched.step(val_loss)\n",
    "        \n",
    "        final_hist['train_loss'].append(tr_loss)\n",
    "        final_hist['train_f1'].append(tr_f1)\n",
    "        final_hist['val_loss'].append(val_loss)\n",
    "        final_hist['val_f1'].append(val_f1)\n",
    "        \n",
    "        if (ep + 1) % 5 == 0:\n",
    "            print(f\"Epoch {ep+1:2d} | Train F1: {tr_f1:.4f} | Val F1: {val_f1:.4f} | Gap: {tr_f1-val_f1:.4f}\")\n",
    "        \n",
    "        if val_f1 > best_val:\n",
    "            best_val = val_f1\n",
    "            counter = 0\n",
    "            best_val_preds = val_preds\n",
    "            best_val_labels = val_labels\n",
    "            torch.save({'model_state_dict': final_model.state_dict(), 'vocab': vocab, \n",
    "                       'label_encoder': le}, os.path.join(save_dir, 'gru_final_model.pt'))\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f\"Early stopping at epoch {ep + 1}\")\n",
    "                break\n",
    "\n",
    "    # Final model plots\n",
    "    plot_final_training_history(final_hist, save_dir)\n",
    "    \n",
    "    # Validation confusion matrix\n",
    "    val_cm_path = os.path.join(save_dir, 'final_validation_confusion_matrix.png')\n",
    "    plot_confusion_matrix_custom(best_val_labels, best_val_preds, le.classes_,\n",
    "                                val_cm_path, title=\"Final Model - Validation Confusion Matrix\")\n",
    "    \n",
    "    # ---- Test Set Evaluation ----\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" EVALUATING ON TEST SET\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    y_test = le.transform(test_df['Label_Multiclass'])\n",
    "    test_loader = DataLoader(HateSpeechDataset(test_df['input_ids'].tolist(), y_test), \n",
    "                            batch_size=64)\n",
    "    \n",
    "    test_loss, test_f1, test_preds, test_labels = evaluate(final_model, test_loader, device, weights)\n",
    "    \n",
    "    # Test confusion matrix\n",
    "    test_cm_path = os.path.join(save_dir, 'final_test_confusion_matrix.png')\n",
    "    plot_confusion_matrix_custom(test_labels, test_preds, le.classes_,\n",
    "                                test_cm_path, title=\"Final Model - Test Set Confusion Matrix\")\n",
    "    \n",
    "    # Final Summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" FINAL SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Cross-Validation F1: {mean_f1:.4f} ± {std_f1:.4f}\")\n",
    "    print(f\"Final Validation F1: {best_val:.4f}\")\n",
    "    print(f\"Test Set F1:         {test_f1:.4f}\")\n",
    "    print(f\"\\nGeneralization: {'✓ Good' if abs(mean_f1 - test_f1) < 0.05 else '⚠ Check variance'}\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    print(\"✓ All visualizations saved to:\", save_dir)\n",
    "    print(\"  - fold_X_history.png (5 files)\")\n",
    "    print(\"  - fold_X_confusion_matrix.png (5 files)\")\n",
    "    print(\"  - cv_f1_summary.png\")\n",
    "    print(\"  - cv_training_summary.png\")\n",
    "    print(\"  - gru_final_training_history.png\")\n",
    "    print(\"  - final_validation_confusion_matrix.png\")\n",
    "    print(\"  - final_test_confusion_matrix.png\")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Entry Point\n",
    "# -------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Adjust paths as needed\n",
    "    train_path = \"/kaggle/input/nephates/train_preprocessed.json\"\n",
    "    val_path = \"/kaggle/input/nephates/val_preprocessed.json\"\n",
    "    test_path = \"/kaggle/input/nephates/test_preprocessed.json\"\n",
    "\n",
    "    train_df = pd.read_json(train_path)\n",
    "    val_df = pd.read_json(val_path)\n",
    "    test_df = pd.read_json(test_path)\n",
    "\n",
    "    print(f\"Data loaded: Train={len(train_df)}, Val={len(val_df)}, Test={len(test_df)}\")\n",
    "    \n",
    "    train_with_cross_validation(train_df, val_df, test_df, n_splits=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
