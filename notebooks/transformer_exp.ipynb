{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f8d2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    TrainingArguments, Trainer, EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, \n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "set_seed(42)\n",
    "\n",
    "print(\"âœ“ All imports successful\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 2: PREPROCESSING UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "import re\n",
    "import emoji\n",
    "import regex\n",
    "from indic_transliteration import sanscript\n",
    "from indic_transliteration.sanscript import transliterate\n",
    "\n",
    "NEPALI_STOPWORDS = set([\n",
    "    \"à¤°\", \"à¤®à¤¾\", \"à¤•à¤¿\", \"à¤­à¤¨à¥‡\", \"à¤¤\", \"à¤›\", \"à¤¹à¥‹\", \"à¤²à¤¾à¤ˆ\", \"à¤²à¥‡\",\n",
    "    \"à¤—à¤°à¥‡à¤•à¥‹\", \"à¤—à¤°à¥à¤›\", \"à¤—à¤°à¥à¤›à¤¨à¥\", \"à¤¹à¥à¤¨à¥\", \"à¤—à¤°à¥‡\", \"à¤¨\", \"à¤¨à¤­à¤à¤•à¥‹\",\n",
    "    \"à¤•à¥‹\", \"à¤•à¤¾\", \"à¤•à¥€\", \"à¤¨à¥‡\", \"à¤ªà¤¨à¤¿\", \"à¤¨à¥ˆ\", \"à¤¥à¤¿à¤¯à¥‹\", \"à¤¥à¤¿à¤\"\n",
    "])\n",
    "\n",
    "DIRGHIKARAN_MAP = {\n",
    "    \"à¤‰\": \"à¤Š\", \"à¤‡\": \"à¤ˆ\", \"à¤‹\": \"à¤°à¤¿\", \"à¤\": \"à¤\", \"à¤…\": \"à¤†\",\n",
    "    \"\\u200d\": \"\", \"\\u200c\": \"\", \"à¥¤\": \".\", \"à¥¥\": \".\",\n",
    "    \"à¤¿\": \"à¥€\", \"à¥\": \"à¥‚\"\n",
    "}\n",
    "\n",
    "def is_devanagari(text: str) -> bool:\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return False\n",
    "    return bool(regex.search(r'\\p{Devanagari}', text))\n",
    "\n",
    "def devanagari_to_roman(text: str) -> str:\n",
    "    try:\n",
    "        return transliterate(text, sanscript.DEVANAGARI, sanscript.ITRANS)\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "def roman_to_devanagari(text: str) -> str:\n",
    "    try:\n",
    "        return transliterate(text, sanscript.ITRANS, sanscript.DEVANAGARI)\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "def normalize_dirghikaran(text: str) -> str:\n",
    "    for original, replacement in DIRGHIKARAN_MAP.items():\n",
    "        text = text.replace(original, replacement)\n",
    "    return text\n",
    "\n",
    "def clean_text(text: str, aggressive: bool = False) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
    "    text = re.sub(r\"@\\w+|#\\w+\", \"\", text)\n",
    "    text = emoji.replace_emoji(text, replace=\"\")\n",
    "    \n",
    "    if aggressive:\n",
    "        text = re.sub(r\"\\d+\", \"\", text)\n",
    "        text = re.sub(r\"[^\\w\\s\\u0900-\\u097F]\", \"\", text)\n",
    "    \n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def preprocess_for_transformer(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    if not is_devanagari(text):\n",
    "        text = roman_to_devanagari(text)\n",
    "    \n",
    "    text = clean_text(text, aggressive=False)\n",
    "    text = normalize_dirghikaran(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "print(\"âœ“ Preprocessing utilities loaded\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 3: FOCAL LOSS FOR HARD EXAMPLES\n",
    "# ============================================================================\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal Loss: Focuses training on hard examples\n",
    "    Paper: https://arxiv.org/abs/1708.02002\n",
    "    \n",
    "    Helps minority classes by down-weighting easy examples\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha  # Class weights\n",
    "        self.gamma = gamma  # Focusing parameter\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "print(\"âœ“ Focal Loss defined\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 4: IMPROVED DATASET WITH STRATIFIED SAMPLING\n",
    "# ============================================================================\n",
    "\n",
    "class NepaliHateDataset(Dataset):\n",
    "    \"\"\"Dataset with better handling of class imbalance\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx]) if self.texts[idx] else \"\"\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 5: CUSTOM TRAINER WITH FOCAL LOSS & PER-CLASS MONITORING\n",
    "# ============================================================================\n",
    "\n",
    "class ImbalancedTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Custom Trainer with:\n",
    "    - Focal Loss\n",
    "    - Per-class F1 monitoring\n",
    "    - Better logging\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, focal_loss_fn=None, label_names=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.focal_loss_fn = focal_loss_fn\n",
    "        self.label_names = label_names or []\n",
    "        self.training_history = {\n",
    "            'train_loss': [],\n",
    "            'eval_loss': [],\n",
    "            'eval_f1_macro': [],\n",
    "            'eval_f1_per_class': {name: [] for name in self.label_names}\n",
    "        }\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        \"\"\"\n",
    "        Compute loss with support for both old and new transformers versions\n",
    "        \"\"\"\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        if self.focal_loss_fn is not None:\n",
    "            loss = self.focal_loss_fn(logits, labels)\n",
    "        else:\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
    "        \"\"\"\n",
    "        Override prediction_step to ensure we compute loss and get predictions\n",
    "        \"\"\"\n",
    "        has_labels = \"labels\" in inputs\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if has_labels:\n",
    "                with self.compute_loss_context_manager():\n",
    "                    loss, outputs = self.compute_loss(model, inputs, return_outputs=True)\n",
    "                loss = loss.mean().detach()\n",
    "            else:\n",
    "                loss = None\n",
    "                outputs = model(**inputs)\n",
    "        \n",
    "        if prediction_loss_only:\n",
    "            return (loss, None, None)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        labels = inputs.get(\"labels\")\n",
    "        \n",
    "        return (loss, logits, labels)\n",
    "    \n",
    "    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix=\"eval\"):\n",
    "        \"\"\"\n",
    "        Override evaluate to ensure metrics are computed\n",
    "        \"\"\"\n",
    "        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n",
    "        eval_dataloader = self.get_eval_dataloader(eval_dataset)\n",
    "        \n",
    "        print(f\"\\n[DEBUG] Starting evaluation with {len(eval_dataset)} samples...\")\n",
    "        \n",
    "        # Run predictions\n",
    "        output = self.prediction_loop(\n",
    "            eval_dataloader,\n",
    "            description=\"Evaluation\",\n",
    "            prediction_loss_only=False,\n",
    "            ignore_keys=ignore_keys,\n",
    "            metric_key_prefix=metric_key_prefix,\n",
    "        )\n",
    "        \n",
    "        print(f\"[DEBUG] Prediction loop completed\")\n",
    "        print(f\"[DEBUG] Output type: {type(output)}\")\n",
    "        print(f\"[DEBUG] Has metrics attr: {hasattr(output, 'metrics')}\")\n",
    "        \n",
    "        # Extract metrics\n",
    "        if hasattr(output, 'metrics'):\n",
    "            metrics = output.metrics\n",
    "        else:\n",
    "            metrics = output if isinstance(output, dict) else {}\n",
    "        \n",
    "        print(f\"[DEBUG] Final metrics keys: {list(metrics.keys())}\")\n",
    "        \n",
    "        # Ensure we have eval_loss\n",
    "        if 'eval_loss' not in metrics and hasattr(output, 'loss'):\n",
    "            metrics['eval_loss'] = output.loss\n",
    "        \n",
    "        self.log(metrics)\n",
    "        self.control = self.callback_handler.on_evaluate(self.args, self.state, self.control, metrics)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def log(self, logs, start_time=None):\n",
    "        \"\"\"\n",
    "        Capture per-class metrics\n",
    "        \"\"\"\n",
    "        # Debug: Print what we're logging\n",
    "        if any('eval' in str(k) for k in logs.keys()):\n",
    "            print(f\"[DEBUG] Logging metrics: {list(logs.keys())}\")\n",
    "        \n",
    "        super().log(logs)\n",
    "        \n",
    "        if 'loss' in logs:\n",
    "            self.training_history['train_loss'].append(logs['loss'])\n",
    "        if 'eval_loss' in logs:\n",
    "            self.training_history['eval_loss'].append(logs['eval_loss'])\n",
    "            print(f\"[DEBUG] Captured eval_loss: {logs['eval_loss']:.4f}\")\n",
    "        if 'eval_f1' in logs:\n",
    "            self.training_history['eval_f1_macro'].append(logs['eval_f1'])\n",
    "            print(f\"[DEBUG] Captured eval_f1: {logs['eval_f1']:.4f}\")\n",
    "        \n",
    "        # Log per-class F1\n",
    "        for i, name in enumerate(self.label_names):\n",
    "            key = f'eval_f1_class_{i}'\n",
    "            if key in logs:\n",
    "                self.training_history['eval_f1_per_class'][name].append(logs[key])\n",
    "\n",
    "print(\"âœ“ ImbalancedTrainer defined with enhanced evaluation\")\n",
    "\n",
    "\n",
    "def compute_metrics_detailed(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute metrics with per-class F1 scores\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    # Debug: Check if function is being called\n",
    "    print(f\"\\n[DEBUG] compute_metrics called with {len(labels)} samples\")\n",
    "    \n",
    "    # Overall metrics\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='macro', zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Per-class F1\n",
    "    precision_per_class, recall_per_class, f1_per_class, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average=None, zero_division=0\n",
    "    )\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': float(accuracy),\n",
    "        'precision': float(precision_macro),\n",
    "        'recall': float(recall_macro),\n",
    "        'f1': float(f1_macro),\n",
    "    }\n",
    "    \n",
    "    # Add per-class F1 to metrics\n",
    "    for i, f1 in enumerate(f1_per_class):\n",
    "        metrics[f'f1_class_{i}'] = float(f1)\n",
    "    \n",
    "    print(f\"[DEBUG] Computed metrics: {list(metrics.keys())}\")\n",
    "    print(f\"[DEBUG] F1 score: {f1_macro:.4f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"âœ“ Custom Trainer and metrics defined\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 6: DATA LOADING & AUGMENTATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" LOADING DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_json(\"/kaggle/input/nepalihate/train.json\")\n",
    "test_df = pd.read_json(\"/kaggle/input/nepalihate/test.json\")\n",
    "\n",
    "# Create validation split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, val_df = train_test_split(\n",
    "    train_df,\n",
    "    test_size=0.15,\n",
    "    stratify=train_df[\"Label_Multiclass\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Data loaded:\")\n",
    "print(f\"  Train: {len(train_df)}\")\n",
    "print(f\"  Val:   {len(val_df)}\")\n",
    "print(f\"  Test:  {len(test_df)}\")\n",
    "\n",
    "# Preprocess\n",
    "print(\"\\nPreprocessing texts...\")\n",
    "for df in [train_df, val_df, test_df]:\n",
    "    df['transformer_input'] = df['Comment'].apply(preprocess_for_transformer)\n",
    "\n",
    "print(\"âœ“ Preprocessing complete\")\n",
    "\n",
    "# Label encoding\n",
    "le = LabelEncoder()\n",
    "le.fit(train_df['Label_Multiclass'])\n",
    "\n",
    "y_train = le.transform(train_df['Label_Multiclass'])\n",
    "y_val = le.transform(val_df['Label_Multiclass'])\n",
    "y_test = le.transform(test_df['Label_Multiclass'])\n",
    "\n",
    "label_names = le.classes_.tolist()\n",
    "num_classes = len(label_names)\n",
    "\n",
    "print(f\"\\nâœ“ Labels encoded: {label_names}\")\n",
    "print(f\"\\nOriginal class distribution:\")\n",
    "for i, name in enumerate(label_names):\n",
    "    count = (y_train == i).sum()\n",
    "    print(f\"  {name}: {count} ({count/len(y_train)*100:.2f}%)\")\n",
    "\n",
    "# CRITICAL: Apply oversampling for minority class\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" APPLYING OVERSAMPLING FOR MINORITY CLASS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "ros = RandomOverSampler(random_state=42, sampling_strategy='not majority')\n",
    "X_train_texts = train_df['transformer_input'].values.reshape(-1, 1)\n",
    "X_train_resampled, y_train_resampled = ros.fit_resample(X_train_texts, y_train)\n",
    "X_train_resampled = X_train_resampled.flatten()\n",
    "\n",
    "print(f\"\\nAfter oversampling:\")\n",
    "for i, name in enumerate(label_names):\n",
    "    count = (y_train_resampled == i).sum()\n",
    "    print(f\"  {name}: {count} ({count/len(y_train_resampled)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nTotal training samples: {len(y_train)} â†’ {len(y_train_resampled)}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 7: COMPUTE ENHANCED CLASS WEIGHTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" COMPUTING CLASS WEIGHTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Compute weights from original distribution (not resampled)\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "# BOOST minority class weight even more\n",
    "minority_class_idx = np.argmin([np.sum(y_train == i) for i in range(num_classes)])\n",
    "class_weights[minority_class_idx] *= 2.0  # Extra boost for OS class\n",
    "\n",
    "# Cap extreme weights\n",
    "class_weights = np.clip(class_weights, 0.5, 10.0)\n",
    "\n",
    "print(f\"\\nClass weights:\")\n",
    "for i, name in enumerate(label_names):\n",
    "    print(f\"  {name}: {class_weights[i]:.2f}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Initialize Focal Loss\n",
    "focal_loss = FocalLoss(alpha=class_weights_tensor, gamma=2.0)\n",
    "print(f\"âœ“ Focal Loss initialized (gamma=2.0)\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 8: CREATE DATASETS & TOKENIZER\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" INITIALIZING MODEL & TOKENIZER\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model_name = 'xlm-roberta-base'\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    add_prefix_space=False\n",
    ")\n",
    "print(f\"âœ“ Tokenizer loaded: {model_name}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = NepaliHateDataset(\n",
    "    X_train_resampled.tolist(),\n",
    "    y_train_resampled,\n",
    "    tokenizer,\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "val_dataset = NepaliHateDataset(\n",
    "    val_df['transformer_input'].tolist(),\n",
    "    y_val,\n",
    "    tokenizer,\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "test_dataset = NepaliHateDataset(\n",
    "    test_df['transformer_input'].tolist(),\n",
    "    y_test,\n",
    "    tokenizer,\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Datasets created:\")\n",
    "print(f\"  Train: {len(train_dataset)}\")\n",
    "print(f\"  Val:   {len(val_dataset)}\")\n",
    "print(f\"  Test:  {len(test_dataset)}\")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_classes,\n",
    "    problem_type=\"single_label_classification\",\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Model loaded: {model_name}\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 9: TRAINING CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "output_dir = '/kaggle/working/xlm_checkpoints'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    \n",
    "    # Training\n",
    "    num_train_epochs=4,  # More epochs for better learning\n",
    "    per_device_train_batch_size=8,  # Smaller batch for better gradients\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=4,  # Effective batch size = 32\n",
    "    \n",
    "    # Optimization\n",
    "    learning_rate=1e-5,  # Lower LR for stability\n",
    "    warmup_ratio=0.15,\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=1.0,\n",
    "    lr_scheduler_type='cosine',\n",
    "    \n",
    "    # Regularization\n",
    "    label_smoothing_factor=0.05,  # Lower smoothing\n",
    "    \n",
    "    # Evaluation - FIXED: Align eval and logging steps\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=100,  # Evaluate every 100 steps\n",
    "    save_strategy='steps',\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    # Logging - CRITICAL: Must match eval_steps for validation loss to show\n",
    "    logging_steps=100,  # Changed to match eval_steps\n",
    "    logging_first_step=True,\n",
    "    report_to='none',\n",
    "    \n",
    "    # Performance\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    dataloader_num_workers=2,\n",
    "    \n",
    "    # Reproducibility\n",
    "    seed=42,\n",
    "    data_seed=42\n",
    ")\n",
    "\n",
    "print(\"âœ“ Training configuration ready (eval and logging aligned at 100 steps)\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 10: INITIALIZE TRAINER & TRAIN\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" STARTING TRAINING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "trainer = ImbalancedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics_detailed,\n",
    "    callbacks=[],  # Removed EarlyStoppingCallback to avoid metric issues\n",
    "    focal_loss_fn=focal_loss,\n",
    "    label_names=label_names\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Trainer initialized\")\n",
    "print(f\"  Compute metrics function: {trainer.compute_metrics is not None}\")\n",
    "print(f\"  Eval dataset size: {len(val_dataset)}\")\n",
    "\n",
    "# Train\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\nâœ“ Training complete!\")\n",
    "print(f\"  Time: {train_result.metrics['train_runtime']:.2f}s\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 11: SAVE MODEL\n",
    "# ============================================================================\n",
    "\n",
    "final_model_dir = '/kaggle/working/xlm_final'\n",
    "os.makedirs(final_model_dir, exist_ok=True)\n",
    "\n",
    "trainer.save_model(final_model_dir)\n",
    "tokenizer.save_pretrained(final_model_dir)\n",
    "\n",
    "import joblib\n",
    "joblib.dump(le, os.path.join(final_model_dir, 'label_encoder.pkl'))\n",
    "\n",
    "print(f\"âœ“ Model saved to: {final_model_dir}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 12: EVALUATION & VISUALIZATION (CLEAN + FIXED)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" VALIDATION EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Run predictions\n",
    "val_predictions = trainer.predict(val_dataset)\n",
    "val_preds = np.argmax(val_predictions.predictions, axis=-1)\n",
    "val_labels = np.array(val_predictions.label_ids)\n",
    "\n",
    "# --- Compute per-class F1 once globally ---\n",
    "precision_all, recall_all, f1_all, support_all = precision_recall_fscore_support(\n",
    "    val_labels, val_preds, average=None, zero_division=0\n",
    ")\n",
    "\n",
    "print(\"\\nPer-Class Validation Results:\")\n",
    "print(\"=\"*70)\n",
    "for i, name in enumerate(label_names):\n",
    "    mask = (val_labels == i)\n",
    "\n",
    "    class_acc = (val_preds[mask] == i).sum() / mask.sum()\n",
    "    class_f1 = f1_all[i]\n",
    "    class_support = support_all[i]\n",
    "\n",
    "    print(f\"{name:20s}: \"\n",
    "          f\"Accuracy={class_acc:.4f}, \"\n",
    "          f\"F1={class_f1:.4f}, \"\n",
    "          f\"Support={class_support}\")\n",
    "\n",
    "print(\"\\n\" + classification_report(\n",
    "    val_labels, val_preds, target_names=label_names, digits=4\n",
    "))\n",
    "\n",
    "# --- Validation Confusion Matrix ---\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(val_labels, val_preds)\n",
    "cm_normalized = cm.astype(float) / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "sns.heatmap(cm_normalized, annot=True, fmt=\".2%\", cmap=\"Blues\",\n",
    "            xticklabels=label_names, yticklabels=label_names)\n",
    "plt.title(\"Validation Confusion Matrix\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/kaggle/working/val_confusion_matrix.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TEST EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" TEST EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Run predictions\n",
    "test_predictions = trainer.predict(test_dataset)\n",
    "test_preds = np.argmax(test_predictions.predictions, axis=-1)\n",
    "test_labels = np.array(test_predictions.label_ids)\n",
    "\n",
    "# Compute per-class once\n",
    "precision_all_t, recall_all_t, f1_all_t, support_all_t = precision_recall_fscore_support(\n",
    "    test_labels, test_preds, average=None, zero_division=0\n",
    ")\n",
    "\n",
    "print(\"\\nPer-Class Test Results:\")\n",
    "print(\"=\"*70)\n",
    "for i, name in enumerate(label_names):\n",
    "    mask = (test_labels == i)\n",
    "\n",
    "    class_acc = (test_preds[mask] == i).sum() / mask.sum()\n",
    "    class_f1 = f1_all_t[i]\n",
    "    class_support = support_all_t[i]\n",
    "\n",
    "    print(f\"{name:20s}: \"\n",
    "          f\"Accuracy={class_acc:.4f}, \"\n",
    "          f\"F1={class_f1:.4f}, \"\n",
    "          f\"Support={class_support}\")\n",
    "\n",
    "print(\"\\n\" + classification_report(\n",
    "    test_labels, test_preds, target_names=label_names, digits=4\n",
    "))\n",
    "\n",
    "# --- Test Confusion Matrix ---\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "cm_normalized = cm.astype(float) / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "sns.heatmap(cm_normalized, annot=True, fmt=\".2%\", cmap=\"Blues\",\n",
    "            xticklabels=label_names, yticklabels=label_names)\n",
    "plt.title(\"Test Confusion Matrix\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/kaggle/working/test_confusion_matrix.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING HISTORY VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(trainer.training_history['train_loss'], label='Train Loss', linewidth=2)\n",
    "\n",
    "if trainer.training_history['eval_loss']:\n",
    "    eval_steps = np.linspace(0, len(trainer.training_history['train_loss']) - 1,\n",
    "                             len(trainer.training_history['eval_loss']))\n",
    "    axes[0].plot(eval_steps, trainer.training_history['eval_loss'],\n",
    "                 label='Val Loss', linewidth=2, marker='o')\n",
    "\n",
    "axes[0].set_title(\"Training Loss\")\n",
    "axes[0].set_xlabel(\"Steps\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].legend()\n",
    "\n",
    "# Macro F1 plot\n",
    "if trainer.training_history['eval_f1_macro']:\n",
    "    eval_steps = np.linspace(0, len(trainer.training_history['train_loss']) - 1,\n",
    "                             len(trainer.training_history['eval_f1_macro']))\n",
    "    axes[1].plot(eval_steps, trainer.training_history['eval_f1_macro'],\n",
    "                 label='Macro F1', linewidth=2, marker='s')\n",
    "\n",
    "axes[1].set_title(\"Validation F1 Score\")\n",
    "axes[1].set_xlabel(\"Steps\")\n",
    "axes[1].set_ylabel(\"F1 Score\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/kaggle/working/training_history.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE FINAL TEST METRICS\n",
    "# ============================================================================\n",
    "\n",
    "test_accuracy = accuracy_score(test_labels, test_preds)\n",
    "test_precision, test_recall, test_f1, _ = precision_recall_fscore_support(\n",
    "    test_labels, test_preds, average=\"macro\", zero_division=0\n",
    ")\n",
    "\n",
    "results = {\n",
    "    \"model\": \"XLM-RoBERTa-Fixed\",\n",
    "    \"test_accuracy\": float(test_accuracy),\n",
    "    \"test_precision\": float(test_precision),\n",
    "    \"test_recall\": float(test_recall),\n",
    "    \"test_f1\": float(test_f1),\n",
    "    \"per_class_f1\": {\n",
    "        label_names[i]: float(f1_all_t[i]) for i in range(len(label_names))\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(\"/kaggle/working/results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" FINAL RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Test Accuracy:  {test_accuracy:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Recall:    {test_recall:.4f}\")\n",
    "print(f\"Test Macro F1:  {test_f1:.4f}\")\n",
    "print(\"\\nPer-Class F1:\")\n",
    "for name, f1 in results[\"per_class_f1\"].items():\n",
    "    print(f\"  {name}: {f1:.4f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "\n",
    "print(\"\\nðŸŽ‰ Training complete! Check /kaggle/working/ for outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63be1397",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
